## Homework 1: What and Why of AI
###### Chris Bollinger
1.
Of the four definitions proposed in Chapter 1, I find the *Acting Rationally* approach most appealing. As an engineer, it is easier to evaluate whether an approach is working if there are good metrics that you can benchmark with. Rational actors always try to achieve the *best possible outcome*, and the ability of an agent to achieve this outcome can be rigorously defined, tested, and compared with the performance of other agents. This makes AI an engineering field, as compared to cognitive neuroscience which studies the process of thinking without building examples of it.

While I think the rational agents paradigm is the most effective way to approach developing AI, I do see one significant downside to this framing of the problem. Because rational agents are evaluated only on their ability to reach the best outcome, the best models may be very difficult for humans to understand, even though they perform rationally. This makes the validation of rationally acting AI systems challenging, since they can approach problem solving completely differently than a human would, and we may not understand why they make the decisions they do. Explainable AI as a field looks to resolve this by adding "explainability" as a feature of the learning systems, but this still seems to be an unsolved problem currently.

2.
John Searle views AI from the perspective of biological naturalism, which suggests that processes happening in biological neurons create mental states as high level emergent features. Because of this, Searle argues that a program that acts intelligently (Weak AI) is not necessarily thinking intelligently (Strong AI) because it replicates intelligent behavior but not the underlying biological processes. Searle's thought experiment for this assertion is the Chinese Room, where a human with no understanding of Chinese uses a "Chinese processing manual" to manipulate and produce correct responses in Chinese.

Searle argues that because the human has no understanding of Chinese, and a manual cannot have understanding, the Chinese Room's mechanical processing of the language is something distinct from when a native Chinese speaker *understands* and responds to a Chinese sentence, even if the behavior is indistinguishable from the outside. Searle thus states that a passed Turing Test does not indicate Strong AI, because agent that are behaviorally identical to humans could just be "Chinese Rooms" that mechanically output a human-identical response, but lack the internal processes that we call intelligent thought.

3.
Article 1: https://www.economist.com/news/special-report/21700758-will-smarter-machines-cause-mass-unemployment-automation-and-anxiety
This article examines the impact of artificial intelligence on employment in the near future. Starting with a discussion of how AI has the potential to automate away white collar and blue collar jobs, with the main factor being whether work is routine. Unfortunately, routine jobs make up nearly 50% of the American economy, in sectors ranging from transportation to sales. Because of this, many experts and authors suggest that the future will present far fewer opportunities for human employment.

However, the Economist also considers historical instances where technological advances seemed the threaten the livelihoods of some workers. Since the Industrial Revolution people have expressed concern that machines will displace workers, and we will run out of jobs. However, in previous periods of advancement this has not been the case. Instead, automation reduces the cost of a product, increasing the demand for it, and thus requiring more workers to continue doing the work that can't be automated. Historically, increasing automation has lead to retraining, not unemployment, so some experts think there is no cause for alarm.
Since recent advances in AI seem more generally capable than previous automation technologies, is it possible we're reached the point where too many things are automated, and people can't just retrain as they have in the past? The article closes on an inconclusive note, without an answer to this question.

I appreciated this article because it does not take an alarmist approach to AI advances, while still having a healthy wariness about what changes could bring if planned poorly. One takeaway I agree with is the need for better educational and retraining options for human workers. Since non-routine, skilled work is the kind of job that AI is least suited to automating, it will be important to train well-rounded, thoughtful individual that can operate as creative guides and leaders for the highly specialized AIs that will continue to be developed. Additionally, I think interpersonal and social skills will be a human element that will be highly valued. If done well, I think this could lead to an economy where more people have interesting, fulfilling jobs instead of ones filled with repetition and tedium. If done poorly, we could see a lot of unemployment, with workers unable to retrain for the new kinds of jobs, and left with few options to support themselves.

Article 2: http://www.defenseone.com/technology/2017/07/russian-weapons-maker-build-ai-guns/139452/
In this article, the arms manufacturer Kalashnikov announced the release of a new weapons system that uses neural AI systems to automatically track and shoot targets. Additionally, there is a discussion about the contrast between the Russian and US positions on automated weapons of war. Historically, Russia has been much more willing to develop AI systems that will use lethal force, where US strategy has been focused on developing tools for augmenting human decision making. The article quotes Deputy of Defense Bob Work, who observed that since authoritarian regimes view people as a threat to their control, they are more likely to develop AI systems that will use lethal force than their democratic government counterparts.

I thought this article was important because it shows that not everyone is equally concerned about the dangers of AI. For some governments, AI and machine learning are attractive tools for building weapons that do exactly what they are designed to do without hesitation or remorse. It is concerning that Russia does not seem to worry about malfunctions, or how to keep these systems from harming people that are not the intended targets. I think that the creation of products like this make movements like the UN resolution to ban automated weaponry even more important, because if there is no action taken, there could be many dangerous AI systems in the world very soon.
